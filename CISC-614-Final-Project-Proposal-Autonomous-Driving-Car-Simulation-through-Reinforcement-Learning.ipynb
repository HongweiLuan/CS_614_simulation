{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Abstract\" data-toc-modified-id=\"Abstract-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Abstract</a></span></li><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Background</a></span></li><li><span><a href=\"#Summary-of-the-published-work​\" data-toc-modified-id=\"Summary-of-the-published-work​-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Summary of the published work​</a></span></li><li><span><a href=\"#Motivation-and-potential-modification\" data-toc-modified-id=\"Motivation-and-potential-modification-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Motivation and potential modification</a></span></li></ul></li><li><span><a href=\"#Preliminary-work-plan\" data-toc-modified-id=\"Preliminary-work-plan-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preliminary work plan</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CISC-614-Final-Project-Autonomous-Driving-Car-Simulation-through-Reinforcement-Learning\n",
    "===\n",
    "\n",
    "* **Hongwei Luan**\n",
    "* **Aug 3th, 2021**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Autonomous driving has been investigated intensively under the help of reinforcement learning in recent years. In this report, a literature review of different components in autonomous driving will be conducted. Then a 2D autonomous vehicle environment, CarRacing-v0, from OpenAI was simulated by applying a double Deep Q Network. The performance will be compared with a published work which used Deep Q network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Background\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In recent years, the field of autonomous vehicles has been a hot research direction, embodied in sensor technology, V2X communication, security protection, decision-making and control, and regulations. In addition to classic control methods, artificial intelligence and machine learning technologies are currently also used in these fields. Some of these researchers focus on different levels of motion planning, such as strategic decision-making, trajectory planning, and control. These problems can be solved using a variety of machine learning techniques espe ially deep reinforcement learning (DRL). \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Since reinforcement learning (RL) is a way to solve complex problems dedicated to achieving general intelligence, it is suitable for solving time series problems, and autonomous driving, as a typical \"industrial artificial intelligence\", contains three major links in the process: perception, decision-making, and control, which need to be considered under various extreme working conditions and scenarios; among them, the environmental elements in the perception process are too complicated, and the driving tasks are flexible and changeable, and there are many non-timing problems. RL can hardly play a major role in them. It is more in the world of deep learning, such as the commonly used YOLO And its variants, etc.; Only at the decision-making level, Model Based RL and Rule based cooperate with each other, in which rule-based construction can cope with most common driving scenarios, but needs to be continuously designed and updated; RL is used to solve extreme scenarios that are not applicable in the rule based questions. Therefore, people usually choose the DRL method, which combines the perception ability of deep learning and the decision-making ability of reinforcement learning, which can be directly based on the input information. Control is an artificial intelligence method that is closer to human thinking, and it is also a mature L4 level solution with more potential.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;However, traditional RL faces some common bottlenecks in solving the problem of autonomous driving. The root cause is the inapplicability of the \"flat\" method. The output of SF (Sensor Fusion) and behavior planning is a multi-dimensional solution problem, and when the multi-dimensional space is \"flattened\", a flat but very large state space is formed, making the original trial and error path very long and tortuous. When backpropagating, the reward signal is too weak. At this time, the process of adjusting the parameters of the algorithm engineer must be tormenting and \"bald\".\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In order to solve the above problems, deep reinforcement learning (DRL) was born spontaneously. It applies deep learning to the expected reward trend prediction. On the one hand, it is helpful to combine with the CV scheme to realize the end to end scheme, and it can also better predict the law in an unknown environment and reduce the expected feedback error.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In this report, a simplified autonomous car racing will be simulated by deep reinforcement learning framework using a classic environment from OpenAI, CarRacing-v0, a 2D autonomous vehicle environment, because the whole autonomous driving is far beyong the scope of this term paper. The purpose of this report is to understand how reinforcement learning help train the autonomous car racing simulation process. \n",
    "\n",
    "\n",
    "### Summary of the published work​\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This published work[1] applied a Deep Q Network for OpenAI’s Car Racing Game using a classic environment from OpenAI, CarRacing-v0, a 2D autonomous vehicle environment. It also explored the use of a Resnet18 pre-trained architecture . The pre-trained model produced more randomized results, while the custom made CNN architecture resulted in a more definite correlation between episodes and rewards. The model was able to regularly surpass a reward count of 350 through the application of this DQN architecture. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Gym envs from OpenAI is an open-source provider of pre-defined simulation environments for artificial intelligence projects. It can utilize various artificial intelligence techniques such as reinforcement learning and computer vision to solve simulation environments. As most simulation environments and solutions are publicly available due to open source, we can use gym environments as benchmark for future developments. The environment explored in this study was CarRacing-v0, a 2D autonomous vehicle environment. We can use a reinforcement learning agent to learn the behavior of a racing car and solve the racing problem.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The specific algorithm used in this study was Deep Q-Network (DQN) from reinforcement learning perspective. It was created on top of Q-Learning which is a simple model-free reinforcement learning algorithm by combining neural networks. For an RL agent to make a decision, it is important for the agent to learn the Q value\n",
    "function. The Q value function can be learned iteratively via Bellman's equation. When the\n",
    "agent starts to interact with the environment, it starts with a random state s(0) and random\n",
    "state of Q values for every state action pair. The agent's action would also be somewhat\n",
    "random, since it has no state Q values to make informed decisions. For each action taken,\n",
    "the environment would return a reward based on which agent starts to build the Q value\n",
    "tables, and improves over time. The agent will update the Q table in such a way that its total long-term\n",
    "expected reward is maximized. Based on the computed long-term reward, the existing Q value of the state action pair $(s^{(t)}, a^{(t)})$ is updated as follows:\n",
    "\n",
    "$$Q^{(t+1)}(s^{(t)}, a^{(t)}) = (1 - \\alpha) * Q^{(t)}(s^{(t)}, a^{(t)}) + \\alpha*r^{'(t)} = (1 - \\alpha) * Q^{(t)}(s^{(t)}, a^{(t)}) + \\alpha*(r^{(t)} + \\gamma \\max_{a^{'}} Q^{(t)}(s^{(t+1)}, a))$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Deep Q learning takes advantage of deep learning networks in learning the Q value function.\n",
    "Illustrated in the following diagram, Figure 1, is the architecture of a deep Q learning\n",
    "network:\n",
    "\n",
    "![deep QN](deep_q_network.png)\n",
    "Figure 1 Deep Q Network\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The deep Q-learning network is trained using a very simple idea, called experience replay.\n",
    "Let the RL agent interact with the environment and use (s, a, r, s') in the replay buffer. You can sample mini-batch from this replay buffer for training the network. In the beginning, the replay buffer is stored with random experience.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Although the published work utilized deep Q network as their reinforcement learning framework, it is outdated up to date. Double DQNs, a modification of DQNs, are typically more efficient. DQNs tend to be more optimistic compared to Double DQNs, which tend to be more skeptical with the actions chosen, calculating the target Q value for taking that action. Overall, Double DQNs help reduce the overestimation of Q-values, allowing training to be faster and have more stable learning. Due to this information, it could be understood that using double learning would have been more effective and beneficial for this environment compared to the classic DQN that was applied.\n",
    "\n",
    "### Motivation and potential modification\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As the autonomous driving is an interesting simulation area which can easily combine with reinforcement learning, it would be a good learning topic for my skill set. I can learn how a real autonomous driving work by reading relevant literatures and apply reinforcement learning on it for practice. Due to the huge complexity of autonomous driving system, I will focus on the car racing environment provided by Open AI for the implementation side. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Therefore, there are two improvements in my report comparing to the published work. First, a literature review of autonomous driving using reinforcement learning in real world will be summarized. Different components in autonomous driving simulation will be compared and investigated. Second, a double deep Q network will be implemented instead of standard deep Q learning network. The performance will be compared with the results from deep Q network. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T20:11:09.634599Z",
     "start_time": "2020-05-28T20:11:09.618321Z"
    }
   },
   "source": [
    "## Preliminary work plan\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In this report, two main sections will be performed. First, a literature review of autonomous driving using reinforcement learning in real world will be collected and summarized. As the autonomous driving involves many stages and very complex such as Perception and Localization, High-Level Path Planning, Behavior Arbitration, or low-level path planning, and Motion Controllers[3], it is difficult to complete the whole simulation of autonomous driving. However, a literature review of all components in autonomous driving is helpful to understand all simulation process and how reinforcement learning is involved in autonomous driving. This is acheivable for a term paper. The timeline for literature review is half semester. Second, a simplified autonomous car racing will be simulated by deep reinforcement learning framework which means double deep Q network using a classic environment from OpenAI, CarRacing-v0, a 2D autonomous vehicle environment. This simulation is much simpler than the whole autonomous driving process. It is more appropriate to be completed in one semester. The simulation work will be focused on the second half of this semester.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Jupyter Notebook will be the major tool to implement the simulation work. The CarRacing-v0 environment from OpenAI will be the fundamental simulation environment in this report. The keras framework from Tensorflow will be used to build the double deep Q neural network. A GPU supported instance may be needed if the model performance is too slow for CPU based instance. Google Colab or AWS p-instance would be the options. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Blog on Towards Data Science “Applying a Deep Q Network for OpenAI’s Car Racing Game”, \n",
    "https://towardsdatascience.com/applying-a-deep-q-network-for-openais-car-racing-game-a642daf58fc9\n",
    "\n",
    "[2] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., & Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236\n",
    "\n",
    "[3] Grigorescu, S., Trasnea, B., Cocias, T., & Macesanu, G. (2019). A survey of deep learning\n",
    "techniques for autonomous driving. Journal of Field Robotics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
